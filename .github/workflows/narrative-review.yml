name: Narrative Chapter Review (PR)

on:
  pull_request:
    types: [opened, synchronize, reopened, edited, labeled, unlabeled]
    paths:
      - "02_NARRATIVE/book_01/CHAPTERS/**"

permissions:
  contents: read
  pull-requests: write
  issues: write

concurrency:
  group: narrative-review-${{ github.event.pull_request.number }}
  cancel-in-progress: true

env:
  MODEL_DEFAULT: "gpt-5-mini"
  MODEL_DEEP: "gpt-5.2"
  DEEP_LABEL: "deep-review"
  TOKEN_LIMIT: "4000"
  TARGET_CHUNK_TOKENS: "3500"
  TARGET_BATCH_TOKENS: "7000"
  CANON_DIGEST_PATH: "00_CANON/_DIGEST.md"
  CANON_DIGEST_MAX_CHARS: "45000"
  MAX_OUTPUT_TOKENS_PER_BATCH: "1400"
  TEMPERATURE: "0.15"

jobs:
  narrative-review:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Ensure jq present
        run: |
          if ! command -v jq >/dev/null 2>&1; then
            sudo apt-get update && sudo apt-get install -y jq
          fi

      - name: Select model
        id: model
        run: |
          LABELS='${{ toJson(github.event.pull_request.labels) }}'
          MODEL="${{ env.MODEL_DEFAULT }}"
          if echo "$LABELS" | grep -q "\"name\":\"${{ env.DEEP_LABEL }}\""; then
            MODEL="${{ env.MODEL_DEEP }}"
          fi
          echo "model=$MODEL" >> "$GITHUB_OUTPUT"
          echo "Using model: $MODEL"

      - name: Collect and chunk files
        id: collect
        env:
          TOKEN_LIMIT: ${{ env.TOKEN_LIMIT }}
          TARGET_CHUNK_TOKENS: ${{ env.TARGET_CHUNK_TOKENS }}
          TARGET_BATCH_TOKENS: ${{ env.TARGET_BATCH_TOKENS }}
        run: |
          set -euo pipefail
          BASE_SHA="${{ github.event.pull_request.base.sha }}"
          HEAD_SHA="${{ github.event.pull_request.head.sha }}"
          
          CHANGED_FILES=$(git diff --name-only "$BASE_SHA" "$HEAD_SHA" -- "02_NARRATIVE/book_01/CHAPTERS/**" | grep -v '^\s*$' || true)
          
          if [[ -z "$CHANGED_FILES" ]]; then
            echo "has_files=false" >> "$GITHUB_OUTPUT"
            echo "No changed chapter files."
            exit 0
          fi
          
          echo "has_files=true" >> "$GITHUB_OUTPUT"
          mkdir -p .tmp_narrative_review
          
          echo "$CHANGED_FILES" > .tmp_narrative_review/files.txt
          
          python3 << 'PYTHON_SCRIPT'
          import os, json, re, pathlib
          
          with open(".tmp_narrative_review/files.txt") as f:
              files = [line.strip() for line in f if line.strip()]
          
          TOKEN_LIMIT = int(os.environ.get("TOKEN_LIMIT", "4000"))
          TARGET_CHUNK = int(os.environ.get("TARGET_CHUNK_TOKENS", "3500"))
          TARGET_BATCH = int(os.environ.get("TARGET_BATCH_TOKENS", "7000"))
          
          outdir = pathlib.Path(".tmp_narrative_review")
          
          def est_tokens(s): return max(1, len(s) // 4)
          def sanitize(name): return re.sub(r"[^A-Za-z0-9._-]+", "_", name)
          
          def chunk_text(text):
              if est_tokens(text) <= TOKEN_LIMIT:
                  return [text]
              lines = text.splitlines(keepends=True)
              chunks, buf, buf_tokens = [], [], 0
              for ln in lines:
                  ln_tokens = est_tokens(ln)
                  is_break = (ln.strip() == "") or ln.lstrip().startswith("#")
                  if buf and (buf_tokens + ln_tokens > TARGET_CHUNK) and is_break:
                      chunks.append("".join(buf).strip("\n") + "\n")
                      buf, buf_tokens = [], 0
                  buf.append(ln)
                  buf_tokens += ln_tokens
                  if buf_tokens > TOKEN_LIMIT:
                      chunks.append("".join(buf).strip("\n") + "\n")
                      buf, buf_tokens = [], 0
              if buf:
                  chunks.append("".join(buf).strip("\n") + "\n")
              return [c for c in chunks if c.strip()] or [text]
          
          manifest_chunks = []
          for f in files:
              p = pathlib.Path(f)
              if not p.exists():
                  continue
              text = p.read_text(encoding="utf-8", errors="ignore")
              parts = chunk_text(text)
              total_est = est_tokens(text)
              for i, part in enumerate(parts, start=1):
                  chunk_path = outdir / f"{sanitize(p.as_posix())}__chunk{i:02d}.txt"
                  chunk_path.write_text(part, encoding="utf-8")
                  manifest_chunks.append({
                      "file": p.as_posix(), "chunk": i, "chunk_count": len(parts),
                      "chunk_path": chunk_path.as_posix(), "est_tokens": est_tokens(part),
                      "est_tokens_total": total_est
                  })
          
          batches = []
          for f in sorted(set(c["file"] for c in manifest_chunks)):
              chunks = sorted([c for c in manifest_chunks if c["file"] == f], key=lambda x: x["chunk"])
              batch, batch_tokens, batch_idx = [], 0, 1
              for c in chunks:
                  if batch and (batch_tokens + c["est_tokens"] > TARGET_BATCH):
                      batches.append({"file": f, "batch": batch_idx, "chunks": batch, "batch_est_tokens": batch_tokens,
                                      "file_est_tokens_total": chunks[0]["est_tokens_total"]})
                      batch, batch_tokens, batch_idx = [], 0, batch_idx + 1
                  batch.append({"chunk": c["chunk"], "chunk_count": c["chunk_count"], "chunk_path": c["chunk_path"], "est_tokens": c["est_tokens"]})
                  batch_tokens += c["est_tokens"]
              if batch:
                  batches.append({"file": f, "batch": batch_idx, "chunks": batch, "batch_est_tokens": batch_tokens,
                                  "file_est_tokens_total": chunks[0]["est_tokens_total"] if chunks else 0})
          
          (outdir / "chunks.json").write_text(json.dumps(manifest_chunks, ensure_ascii=False, indent=2))
          (outdir / "batches.json").write_text(json.dumps(batches, ensure_ascii=False, indent=2))
          print(f"Files: {len(set(c['file'] for c in manifest_chunks))}, Chunks: {len(manifest_chunks)}, Batches: {len(batches)}")
          PYTHON_SCRIPT

      - name: Run review
        if: steps.collect.outputs.has_files == 'true'
        env:
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          MODEL: ${{ steps.model.outputs.model }}
          GH_TOKEN: ${{ github.token }}
          TEMPERATURE: ${{ env.TEMPERATURE }}
          MAX_OUTPUT_TOKENS: ${{ env.MAX_OUTPUT_TOKENS_PER_BATCH }}
        run: |
          set -euo pipefail
          
          CANON_CTX=""
          if [[ -f "${{ env.CANON_DIGEST_PATH }}" ]]; then
            CANON_CTX=$(head -c ${{ env.CANON_DIGEST_MAX_CHARS }} "${{ env.CANON_DIGEST_PATH }}" || true)
          fi
          
          PR_NUMBER="${{ github.event.pull_request.number }}"
          COMMENT_FILE=".tmp_narrative_review/review_comment.md"
          QUOTA_EXCEEDED="false"
          
          cat > "$COMMENT_FILE" << 'HEADER'
          <!-- NARRATIVE_REVIEW -->
          # Narrative Review (AI)
          HEADER
          
          echo "" >> "$COMMENT_FILE"
          echo "- Model: \`${MODEL}\`" >> "$COMMENT_FILE"
          echo "- Scope: \`02_NARRATIVE/book_01/CHAPTERS/**\`" >> "$COMMENT_FILE"
          echo "- PR: #${PR_NUMBER}" >> "$COMMENT_FILE"
          echo "" >> "$COMMENT_FILE"
          
          BATCH_COUNT=$(jq 'length' .tmp_narrative_review/batches.json)
          
          for idx in $(seq 0 $((BATCH_COUNT-1))); do
            [[ "$QUOTA_EXCEEDED" == "true" ]] && break
            
            FILE=$(jq -r ".[$idx].file" .tmp_narrative_review/batches.json)
            BATCH=$(jq -r ".[$idx].batch" .tmp_narrative_review/batches.json)
            BATCHTOK=$(jq -r ".[$idx].batch_est_tokens" .tmp_narrative_review/batches.json)
            
            # Build text block from chunks
            TEXT_BLOCK=""
            CHUNK_COUNT=$(jq -r ".[$idx].chunks | length" .tmp_narrative_review/batches.json)
            for cidx in $(seq 0 $((CHUNK_COUNT-1))); do
              CHUNK_PATH=$(jq -r ".[$idx].chunks[$cidx].chunk_path" .tmp_narrative_review/batches.json)
              CHUNK_NUM=$(jq -r ".[$idx].chunks[$cidx].chunk" .tmp_narrative_review/batches.json)
              CHUNK_TOTAL=$(jq -r ".[$idx].chunks[$cidx].chunk_count" .tmp_narrative_review/batches.json)
              CHUNK_TEXT=$(cat "$CHUNK_PATH")
              TEXT_BLOCK="${TEXT_BLOCK}
          
          ---
          [CHUNK ${CHUNK_NUM}/${CHUNK_TOTAL}]
          
          ${CHUNK_TEXT}"
            done
            
            PROMPT="SYSTEM:
          Du bist ein strenger Lektor für ein deutsches YA-Fantasy-Franchise.
          Reviewe den Kapiteltext. Keine Neuschreibung, nur konkrete Verbesserungsvorschläge.
          
          Ausgabeformat:
          1) Kurzfazit (2-4 Sätze)
          2) BLOCKER (falls vorhanden)
          3) WICHTIG
          4) NITPICK
          5) Canon-Risiken
          6) Fragen an Autor
          
          CANON CONTEXT:
          ${CANON_CTX}
          
          USER:
          Datei: ${FILE}
          Batch: ${BATCH} (est. ${BATCHTOK} tokens)
          ---
          ${TEXT_BLOCK}"
            
            PAYLOAD=$(jq -n \
              --arg model "$MODEL" \
              --arg prompt "$PROMPT" \
              --argjson temp "${TEMPERATURE}" \
              --argjson maxout "${MAX_OUTPUT_TOKENS}" \
              '{model: $model, input: [{role:"user", content:[{type:"text", text:$prompt}]}], temperature: $temp, max_output_tokens: $maxout}')
            
            HTTP_CODE=$(curl -sS -w "%{http_code}" -o resp.json \
              -H "Authorization: Bearer $OPENAI_API_KEY" \
              -H "Content-Type: application/json" \
              https://api.openai.com/v1/responses \
              -d "$PAYLOAD")
            
            if [[ "$HTTP_CODE" -ge 400 ]]; then
              ERR_CODE=$(jq -r '.error.code // empty' resp.json)
              ERR_MSG=$(jq -r '.error.message // empty' resp.json)
              
              if [[ "$HTTP_CODE" == "429" && "$ERR_CODE" == "insufficient_quota" ]]; then
                echo -e "\n## OpenAI Quota\n\n**Quota exhausted** (HTTP 429). Bitte Billing prüfen.\n\n> ${ERR_MSG}" >> "$COMMENT_FILE"
                QUOTA_EXCEEDED="true"
                continue
              fi
              
              echo -e "\n## ${FILE} — Batch ${BATCH}\n\n**API Error** (HTTP ${HTTP_CODE}) \`${ERR_CODE}\`\n\n> ${ERR_MSG}" >> "$COMMENT_FILE"
              continue
            fi
            
            OUT=$(jq -r 'if .output_text then .output_text else ([.output[]?.content[]? | select(.type=="output_text" or .type=="text") | .text] | join("\n")) end' resp.json)
            
            echo -e "\n## ${FILE} — Batch ${BATCH}\n\n${OUT}" >> "$COMMENT_FILE"
          done
          
          # Truncate if too long
          if [[ $(wc -c < "$COMMENT_FILE") -gt 60000 ]]; then
            head -c 60000 "$COMMENT_FILE" > "${COMMENT_FILE}.tmp"
            echo -e "\n\n---\n\n(Truncated)" >> "${COMMENT_FILE}.tmp"
            mv "${COMMENT_FILE}.tmp" "$COMMENT_FILE"
          fi

      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: narrative-review
          path: .tmp_narrative_review/

      - name: Upsert PR comment
        if: steps.collect.outputs.has_files == 'true'
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          PR_NUMBER="${{ github.event.pull_request.number }}"
          REPO="${{ github.repository }}"
          BODY_FILE=".tmp_narrative_review/review_comment.md"
          
          [[ ! -f "$BODY_FILE" ]] && exit 0
          
          COMMENT_ID=$(gh api "repos/${REPO}/issues/${PR_NUMBER}/comments" --paginate | jq -r '.[] | select(.body | contains("<!-- NARRATIVE_REVIEW -->")) | .id' | head -n 1 || true)
          
          if [[ -n "$COMMENT_ID" && "$COMMENT_ID" != "null" ]]; then
            gh api -X PATCH "repos/${REPO}/issues/comments/${COMMENT_ID}" -f "body=@${BODY_FILE}"
          else
            gh api -X POST "repos/${REPO}/issues/${PR_NUMBER}/comments" -f "body=@${BODY_FILE}"
          fi
